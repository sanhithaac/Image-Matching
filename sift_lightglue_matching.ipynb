{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65081e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "045436c2",
   "metadata": {},
   "source": [
    "# **Image Matching Pipeline using SIFT + LightGlue**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c956f6d",
   "metadata": {},
   "source": [
    "Here's a detailed explanation of your **image matching pipeline** using SIFT + LightGlue in PyTorch/Kornia:\n",
    "\n",
    "---\n",
    "\n",
    "### **1ï¸âƒ£ Imports and Setup**\n",
    "\n",
    "```python\n",
    "import os, io, requests, cv2, torch, kornia as K, kornia.feature as KF\n",
    "from kornia_moons.feature import laf_from_opencv_SIFT_kpts\n",
    "from kornia_moons.viz import draw_LAF_matches\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "* **OpenCV (cv2)**: for SIFT keypoint detection.\n",
    "* **Kornia / Kornia-Moons**: differentiable computer vision in PyTorch. Includes LightGlue matcher, LAF utilities, and visualization helpers.\n",
    "* **Torch**: core tensor operations, device handling.\n",
    "* **Matplotlib**: visualize matches.\n",
    "\n",
    "---\n",
    "\n",
    "### **2ï¸âƒ£ Device Selection**\n",
    "\n",
    "```python\n",
    "compute_device = K.utils.get_cuda_or_mps_device_if_available()\n",
    "print(compute_device)\n",
    "```\n",
    "\n",
    "* Chooses GPU if available (CUDA or Apple MPS).\n",
    "* Speeds up descriptor matching and LightGlue computations.\n",
    "\n",
    "---\n",
    "\n",
    "### **3ï¸âƒ£ LightGlue Matcher and SIFT Initialization**\n",
    "\n",
    "```python\n",
    "lightglue_model = KF.LightGlueMatcher(\"sift\").eval().to(compute_device)\n",
    "max_keypoints = 4096\n",
    "sift_detector = cv2.SIFT_create(max_keypoints)\n",
    "```\n",
    "\n",
    "* **LightGlueMatcher(\"sift\")**: pretrained model for SIFT descriptors.\n",
    "* **SIFT_create(4096)**: detects up to 4096 features per image.\n",
    "\n",
    "---\n",
    "\n",
    "### **4ï¸âƒ£ Image Loading**\n",
    "\n",
    "```python\n",
    "image_a = cv2.cvtColor(cv2.imread(img_path_1), cv2.COLOR_BGR2RGB)\n",
    "image_b = cv2.cvtColor(cv2.imread(img_path_2), cv2.COLOR_BGR2RGB)\n",
    "dims_a = torch.tensor(image_a.shape[:2], device=compute_device)\n",
    "dims_b = torch.tensor(image_b.shape[:2], device=compute_device)\n",
    "```\n",
    "\n",
    "* Reads and converts OpenCV BGR â†’ RGB.\n",
    "* Stores height/width for LightGlue scaling.\n",
    "\n",
    "---\n",
    "\n",
    "### **5ï¸âƒ£ Descriptor Normalization: RootSIFT**\n",
    "\n",
    "```python\n",
    "def convert_to_rootsift(desc: torch.Tensor, epsilon=1e-6) -> torch.Tensor:\n",
    "    desc = torch.nn.functional.normalize(desc, p=1, dim=-1, eps=epsilon)\n",
    "    desc.clip_(min=epsilon).sqrt_()\n",
    "    return torch.nn.functional.normalize(desc, p=2, dim=-1, eps=epsilon)\n",
    "```\n",
    "\n",
    "* **RootSIFT** improves SIFT matching by:\n",
    "\n",
    "  1. L1-normalizing the descriptor.\n",
    "  2. Taking square root (Hellinger kernel).\n",
    "  3. L2-normalizing again.\n",
    "\n",
    "---\n",
    "\n",
    "### **6ï¸âƒ£ Detect Keypoints and Compute Descriptors**\n",
    "\n",
    "```python\n",
    "with torch.inference_mode():\n",
    "    keypoints_a, descriptors_a = sift_detector.detectAndCompute(image_a, None)\n",
    "    keypoints_b, descriptors_b = sift_detector.detectAndCompute(image_b, None)\n",
    "```\n",
    "\n",
    "* Uses OpenCV SIFT to extract **keypoints** and **descriptors**.\n",
    "\n",
    "```python\n",
    "frames_a = laf_from_opencv_SIFT_kpts(keypoints_a, compute_device)\n",
    "frames_b = laf_from_opencv_SIFT_kpts(keypoints_b, compute_device)\n",
    "descriptors_a = convert_to_rootsift(torch.from_numpy(descriptors_a)).to(compute_device)\n",
    "descriptors_b = convert_to_rootsift(torch.from_numpy(descriptors_b)).to(compute_device)\n",
    "```\n",
    "\n",
    "* Converts OpenCV keypoints â†’ **Local Affine Frames (LAFs)** compatible with Kornia.\n",
    "* Converts descriptors to PyTorch and applies **RootSIFT**.\n",
    "\n",
    "---\n",
    "\n",
    "### **7ï¸âƒ£ LightGlue Matching**\n",
    "\n",
    "```python\n",
    "distances, match_indices = lightglue_model(descriptors_a, descriptors_b, frames_a, frames_b, hw1=dims_a, hw2=dims_b)\n",
    "```\n",
    "\n",
    "* LightGlue produces **tentative matches** between two sets of descriptors using attention-based matching.\n",
    "* `match_indices` contains matched indices: `(idx_image_a, idx_image_b)`.\n",
    "\n",
    "```python\n",
    "print(f\"{match_indices.shape[0]} tentative matches with SIFT-LightGlue\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8ï¸âƒ£ Extract Matched Keypoints**\n",
    "\n",
    "```python\n",
    "def extract_matched_points(points_a, points_b, match_indices):\n",
    "    matched_pts_a = points_a[match_indices[:, 0]]\n",
    "    matched_pts_b = points_b[match_indices[:, 1]]\n",
    "    return matched_pts_a, matched_pts_b\n",
    "\n",
    "matched_pts_a, matched_pts_b = extract_matched_points(KF.get_laf_center(frames_a)[0], KF.get_laf_center(frames_b)[0], match_indices.detach().cpu())\n",
    "```\n",
    "\n",
    "* Converts LAFs â†’ **keypoint centers**.\n",
    "* Retrieves matched coordinates for both images.\n",
    "\n",
    "---\n",
    "\n",
    "### **9ï¸âƒ£ Estimate Fundamental Matrix + Inliers**\n",
    "\n",
    "```python\n",
    "fundamental_mat, inlier_mask = cv2.findFundamentalMat(\n",
    "    matched_pts_a.detach().cpu().numpy(), matched_pts_b.detach().cpu().numpy(),\n",
    "    cv2.USAC_MAGSAC, 1.5, 0.999, 100000\n",
    ")\n",
    "inlier_mask = inlier_mask > 0\n",
    "```\n",
    "\n",
    "* **RANSAC-based fundamental matrix estimation**.\n",
    "* Filters out outliers to keep only **geometrically consistent matches**.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ”Ÿ Visualize Matches**\n",
    "\n",
    "```python\n",
    "draw_LAF_matches(\n",
    "    frames_a, frames_b,\n",
    "    match_indices.cpu().numpy(),\n",
    "    image_a, image_b,\n",
    "    inlier_mask,\n",
    "    draw_dict={\n",
    "        \"inlier_color\": (0.2, 1, 0.2),\n",
    "        \"tentative_color\": (1, 1, 0.2, 0.3),\n",
    "        \"feature_color\": None,\n",
    "        \"vertical\": False\n",
    "    },\n",
    ")\n",
    "plt.title(f\"{inlier_mask.sum()} inliers with SIFT-LightGlue\")\n",
    "```\n",
    "\n",
    "* Draws tentative matches (yellow) and **inliers** (green) between images.\n",
    "* LAFs allow visualization of **keypoint affine frames**, not just points.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Pipeline Summary**\n",
    "\n",
    "1. Load images and initialize SIFT.\n",
    "2. Detect keypoints & compute SIFT descriptors.\n",
    "3. Convert keypoints â†’ LAFs (affine frames).\n",
    "4. Convert descriptors â†’ RootSIFT + PyTorch tensors.\n",
    "5. Run **LightGlue** to get tentative matches.\n",
    "6. Extract keypoint coordinates from LAFs.\n",
    "7. Compute **fundamental matrix** to filter inliers.\n",
    "8. Visualize matches with LAFs and inlier overlay.\n",
    "\n",
    "This approach combines **classic SIFT features** with **modern LightGlue attention-based matching**, giving robust matches even under scale, rotation, and lighting changes.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also **draw a step-by-step diagram** showing this pipeline visually from images â†’ SIFT â†’ LightGlue â†’ inliers. It helps a lot for presentations. Do you want me to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LightGlue via kornia requires kornia>=0.7.2\n",
    "!pip install kornia==0.7.2\n",
    "!pip install kornia_moons --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p .cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/lightglue/pytorch/sift/1/sift_lightglue.pth .cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import requests\n",
    "import cv2\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from kornia_moons.feature import laf_from_opencv_SIFT_kpts\n",
    "from kornia_moons.viz import draw_LAF_matches\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bac8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_1 = \"/kaggle/input/image-matching-challenge-2024/train/church/images/00017.png\"\n",
    "img_path_2 = \"/kaggle/input/image-matching-challenge-2024/train/church/images/00020.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_rootsift(desc: torch.Tensor, epsilon=1e-6) -> torch.Tensor:\n",
    "    desc = torch.nn.functional.normalize(desc, p=1, dim=-1, eps=epsilon)\n",
    "    desc.clip_(min=epsilon).sqrt_()\n",
    "    return torch.nn.functional.normalize(desc, p=2, dim=-1, eps=epsilon)\n",
    "\n",
    "compute_device = K.utils.get_cuda_or_mps_device_if_available()\n",
    "print(compute_device)\n",
    "\n",
    "\n",
    "lightglue_model = KF.LightGlueMatcher(\"sift\").eval().to(compute_device)\n",
    "\n",
    "max_keypoints = 4096\n",
    "sift_detector = cv2.SIFT_create(max_keypoints)\n",
    "\n",
    "image_a = cv2.cvtColor(cv2.imread(img_path_1), cv2.COLOR_BGR2RGB)\n",
    "image_b = cv2.cvtColor(cv2.imread(img_path_2), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "dims_a = torch.tensor(image_a.shape[:2], device=compute_device)\n",
    "dims_b = torch.tensor(image_b.shape[:2], device=compute_device)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    keypoints_a, descriptors_a = sift_detector.detectAndCompute(image_a, None)\n",
    "    keypoints_b, descriptors_b = sift_detector.detectAndCompute(image_b, None)\n",
    "    frames_a = laf_from_opencv_SIFT_kpts(keypoints_a, compute_device)\n",
    "    frames_b = laf_from_opencv_SIFT_kpts(keypoints_b, compute_device)\n",
    "    descriptors_a = convert_to_rootsift(torch.from_numpy(descriptors_a)).to(compute_device)\n",
    "    descriptors_b = convert_to_rootsift(torch.from_numpy(descriptors_b)).to(compute_device)\n",
    "    distances, match_indices = lightglue_model(descriptors_a, descriptors_b, frames_a, frames_b, hw1=dims_a, hw2=dims_b)\n",
    "\n",
    "print(f\"{match_indices.shape[0]} tentative matches with SIFT-LightGlue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "preview_a = Image.open(img_path_1)\n",
    "preview_b = Image.open(img_path_2)\n",
    "display(preview_a)\n",
    "display(preview_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matched_points(points_a, points_b, match_indices):\n",
    "    matched_pts_a = points_a[match_indices[:, 0]]\n",
    "    matched_pts_b = points_b[match_indices[:, 1]]\n",
    "    return matched_pts_a, matched_pts_b\n",
    "\n",
    "\n",
    "matched_pts_a, matched_pts_b = extract_matched_points(KF.get_laf_center(frames_a)[0], KF.get_laf_center(frames_b)[0], match_indices.detach().cpu())\n",
    "\n",
    "fundamental_mat, inlier_mask = cv2.findFundamentalMat(\n",
    "    matched_pts_a.detach().cpu().numpy(), matched_pts_b.detach().cpu().numpy(), cv2.USAC_MAGSAC, 1.5, 0.999, 100000\n",
    ")\n",
    "inlier_mask = inlier_mask > 0\n",
    "\n",
    "draw_LAF_matches(\n",
    "    frames_a, \n",
    "    frames_b,\n",
    "    match_indices.cpu().numpy(),\n",
    "    image_a,\n",
    "    image_b,\n",
    "    inlier_mask,\n",
    "    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \n",
    "               \"tentative_color\": (1, 1, 0.2, 0.3), \n",
    "               \"feature_color\": None, \n",
    "               \"vertical\": True},\n",
    ")\n",
    "plt.title(f\"{inlier_mask.sum()} inliers with SIFT-LightGlue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2673b",
   "metadata": {},
   "source": [
    "    Inliers\n",
    "    Correct correspondences (shown in green)\n",
    "    Reliable correspondences that are geometrically consistent\n",
    "\n",
    "    Outliers\n",
    "    Incorrect correspondences (remain yellow)\n",
    "    Incorrect correspondences that are geometrically inconsistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Second image pair ---\n",
    "img_path_3 = \"/kaggle/input/image-matching-challenge-2024/train/church/images/00021.png\"\n",
    "img_path_4 = \"/kaggle/input/image-matching-challenge-2024/train/church/images/00022.png\"\n",
    "\n",
    "image_c = cv2.cvtColor(cv2.imread(img_path_3), cv2.COLOR_BGR2RGB)\n",
    "image_d = cv2.cvtColor(cv2.imread(img_path_4), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "dims_c = torch.tensor(image_c.shape[:2], device=compute_device)\n",
    "dims_d = torch.tensor(image_d.shape[:2], device=compute_device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    keypoints_c, descriptors_c = sift_detector.detectAndCompute(image_c, None)\n",
    "    keypoints_d, descriptors_d = sift_detector.detectAndCompute(image_d, None)\n",
    "    frames_c = laf_from_opencv_SIFT_kpts(keypoints_c, compute_device)\n",
    "    frames_d = laf_from_opencv_SIFT_kpts(keypoints_d, compute_device)\n",
    "    descriptors_c = convert_to_rootsift(torch.from_numpy(descriptors_c)).to(compute_device)\n",
    "    descriptors_d = convert_to_rootsift(torch.from_numpy(descriptors_d)).to(compute_device)\n",
    "    distances_cd, match_indices_cd = lightglue_model(descriptors_c, descriptors_d, frames_c, frames_d, hw1=dims_c, hw2=dims_d)\n",
    "\n",
    "print(f\"{match_indices_cd.shape[0]} tentative matches with SIFT-LightGlue (pair 2)\")\n",
    "\n",
    "matched_pts_c, matched_pts_d = extract_matched_points(KF.get_laf_center(frames_c)[0], KF.get_laf_center(frames_d)[0], match_indices_cd.detach().cpu())\n",
    "\n",
    "fundamental_mat_cd, inlier_mask_cd = cv2.findFundamentalMat(\n",
    "    matched_pts_c.detach().cpu().numpy(), matched_pts_d.detach().cpu().numpy(), cv2.USAC_MAGSAC, 1.5, 0.999, 100000\n",
    ")\n",
    "inlier_mask_cd = inlier_mask_cd > 0\n",
    "\n",
    "draw_LAF_matches(\n",
    "    frames_c, \n",
    "    frames_d,\n",
    "    match_indices_cd.cpu().numpy(),\n",
    "    image_c,\n",
    "    image_d,\n",
    "    inlier_mask_cd,\n",
    "    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \n",
    "               \"tentative_color\": (1, 1, 0.2, 0.3), \n",
    "               \"feature_color\": None, \n",
    "               \"vertical\": True},\n",
    ")\n",
    "plt.title(f\"{inlier_mask_cd.sum()} inliers with SIFT-LightGlue (pair 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd86fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Third image pair ---\n",
    "img_path_5 = \"/kaggle/input/image-matching-challenge-2024/train/church/images/00023.png\"\n",
    "img_path_6 = \"/kaggle/input/image-matching-challenge-2024/train/church/images/00024.png\"\n",
    "\n",
    "image_e = cv2.cvtColor(cv2.imread(img_path_5), cv2.COLOR_BGR2RGB)\n",
    "image_f = cv2.cvtColor(cv2.imread(img_path_6), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "dims_e = torch.tensor(image_e.shape[:2], device=compute_device)\n",
    "dims_f = torch.tensor(image_f.shape[:2], device=compute_device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    keypoints_e, descriptors_e = sift_detector.detectAndCompute(image_e, None)\n",
    "    keypoints_f, descriptors_f = sift_detector.detectAndCompute(image_f, None)\n",
    "    frames_e = laf_from_opencv_SIFT_kpts(keypoints_e, compute_device)\n",
    "    frames_f = laf_from_opencv_SIFT_kpts(keypoints_f, compute_device)\n",
    "    descriptors_e = convert_to_rootsift(torch.from_numpy(descriptors_e)).to(compute_device)\n",
    "    descriptors_f = convert_to_rootsift(torch.from_numpy(descriptors_f)).to(compute_device)\n",
    "    distances_ef, match_indices_ef = lightglue_model(descriptors_e, descriptors_f, frames_e, frames_f, hw1=dims_e, hw2=dims_f)\n",
    "\n",
    "print(f\"{match_indices_ef.shape[0]} tentative matches with SIFT-LightGlue (pair 3)\")\n",
    "\n",
    "matched_pts_e, matched_pts_f = extract_matched_points(KF.get_laf_center(frames_e)[0], KF.get_laf_center(frames_f)[0], match_indices_ef.detach().cpu())\n",
    "\n",
    "fundamental_mat_ef, inlier_mask_ef = cv2.findFundamentalMat(\n",
    "    matched_pts_e.detach().cpu().numpy(), matched_pts_f.detach().cpu().numpy(), cv2.USAC_MAGSAC, 1.5, 0.999, 100000\n",
    ")\n",
    "inlier_mask_ef = inlier_mask_ef > 0\n",
    "\n",
    "draw_LAF_matches(\n",
    "    frames_e, \n",
    "    frames_f,\n",
    "    match_indices_ef.cpu().numpy(),\n",
    "    image_e,\n",
    "    image_f,\n",
    "    inlier_mask_ef,\n",
    "    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \n",
    "               \"tentative_color\": (1, 1, 0.2, 0.3), \n",
    "               \"feature_color\": None, \n",
    "               \"vertical\": True},\n",
    ")\n",
    "plt.title(f\"{inlier_mask_ef.sum()} inliers with SIFT-LightGlue (pair 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ec855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fourth image pair ---\n",
    "img_path_7 = \"/kaggle/input/image-matching-challenge-2024/train/church/images/00025.png\"\n",
    "img_path_8 = \"/kaggle/input/image-matching-challenge-2024/train/church/images/00026.png\"\n",
    "\n",
    "image_g = cv2.cvtColor(cv2.imread(img_path_7), cv2.COLOR_BGR2RGB)\n",
    "image_h = cv2.cvtColor(cv2.imread(img_path_8), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "dims_g = torch.tensor(image_g.shape[:2], device=compute_device)\n",
    "dims_h = torch.tensor(image_h.shape[:2], device=compute_device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    keypoints_g, descriptors_g = sift_detector.detectAndCompute(image_g, None)\n",
    "    keypoints_h, descriptors_h = sift_detector.detectAndCompute(image_h, None)\n",
    "    frames_g = laf_from_opencv_SIFT_kpts(keypoints_g, compute_device)\n",
    "    frames_h = laf_from_opencv_SIFT_kpts(keypoints_h, compute_device)\n",
    "    descriptors_g = convert_to_rootsift(torch.from_numpy(descriptors_g)).to(compute_device)\n",
    "    descriptors_h = convert_to_rootsift(torch.from_numpy(descriptors_h)).to(compute_device)\n",
    "    distances_gh, match_indices_gh = lightglue_model(descriptors_g, descriptors_h, frames_g, frames_h, hw1=dims_g, hw2=dims_h)\n",
    "\n",
    "print(f\"{match_indices_gh.shape[0]} tentative matches with SIFT-LightGlue (pair 4)\")\n",
    "\n",
    "matched_pts_g, matched_pts_h = extract_matched_points(KF.get_laf_center(frames_g)[0], KF.get_laf_center(frames_h)[0], match_indices_gh.detach().cpu())\n",
    "\n",
    "fundamental_mat_gh, inlier_mask_gh = cv2.findFundamentalMat(\n",
    "    matched_pts_g.detach().cpu().numpy(), matched_pts_h.detach().cpu().numpy(), cv2.USAC_MAGSAC, 1.5, 0.999, 100000\n",
    ")\n",
    "inlier_mask_gh = inlier_mask_gh > 0\n",
    "\n",
    "draw_LAF_matches(\n",
    "    frames_g, \n",
    "    frames_h,\n",
    "    match_indices_gh.cpu().numpy(),\n",
    "    image_g,\n",
    "    image_h,\n",
    "    inlier_mask_gh,\n",
    "    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \n",
    "               \"tentative_color\": (1, 1, 0.2, 0.3), \n",
    "               \"feature_color\": None, \n",
    "               \"vertical\": True},\n",
    ")\n",
    "plt.title(f\"{inlier_mask_gh.sum()} inliers with SIFT-LightGlue (pair 4)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
